# llm/llm_runner.py

from __future__ import annotations
import os
import logging
from typing import Any, Tuple

# â”€â”€ ã¾ãšãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒå¤‰æ•°ã‚’ã‚¯ãƒªã‚¢ã—ã¦ãŠã â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("==== [llm_runner] DEBUG: OPENAI_API_KEY =", (os.environ.get("OPENAI_API_KEY") or "")[:10], "****")
print("==== [llm_runner] DEBUG: USE_LOCAL_LLM =", os.environ.get("USE_LOCAL_LLM"))

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline,
)
from langchain_community.llms import HuggingFacePipeline

# LangChain 0.2.x ç³»ã§ã¯ langchain_community.chat_models ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã‹ã€
# ã¾ãŸã¯åˆ¥é€” pip install langchain-openai ã—ã¦ã„ã‚‹å ´åˆã¯ langchain_openai ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹
# ã“ã“ã§ã¯ langchain-openai ã‚’ä½¿ã†æƒ³å®šã§æ›¸ã„ã¦ã„ã¾ã™ã€‚
try:
    # å„ªå…ˆï¼š langchain_openai ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆ
    from langchain_openai import ChatOpenAI
except ImportError:
    # pip install langchain-openai ã‚’è¡Œã£ã¦ã„ãªã„å ´åˆã¯ã“ã¡ã‚‰
    from langchain_community.chat_models import ChatOpenAI

logger = logging.getLogger(__name__)


def _load_local_rinna() -> Tuple[Any, Any, int]:
    """
    rinna/japanese-gpt-neox-3.6b-instruction-ppo ãƒ¢ãƒ‡ãƒ«ã‚’ 8bit or float16 ã§ãƒ­ãƒ¼ãƒ‰ã—ã€
    HuggingFacePipeline ã«å¤‰æ›ã—ã¦è¿”ã™ã€‚
    """
    model_name = "rinna/japanese-gpt-neox-3.6b-instruction-ppo"
    cache_dir = os.getenv("HF_HOME", "/tmp/huggingface")
    max_new_tokens = int(os.getenv("MAX_NEW_TOKENS", 256))

    tokenizer = AutoTokenizer.from_pretrained(
        model_name, use_fast=False, trust_remote_code=True, cache_dir=cache_dir
    )
    try:
        logger.info("ğŸ§  Loading rinna 3.6B in 8-bit")
        bnb_cfg = BitsAndBytesConfig(load_in_8bit=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            quantization_config=bnb_cfg,
            trust_remote_code=True,
            cache_dir=cache_dir,
        )
    except Exception as e:
        logger.warning(f"âš ï¸ 8-bit load failed: {e} / fallback to float16")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto",
            trust_remote_code=True,
            cache_dir=cache_dir,
        )

    gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.95,
        repetition_penalty=1.1,
    )
    llm = HuggingFacePipeline(pipeline=gen_pipe)
    return llm, tokenizer, max_new_tokens


def load_llm() -> Tuple[Any, Any | None, int]:
    """
    USE_LOCAL_LLM ã¨ MODEL_PRESET ã«å¿œã˜ã¦ LLMï¼ˆOpenAI ã‹ Rinnaï¼‰ã‚’è¿”ã™ã€‚
    ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒå¤‰æ•°ã¯ã™ã§ã«ãƒ•ã‚¡ã‚¤ãƒ«å†’é ­ã§ã‚¯ãƒªã‚¢æ¸ˆã¿ãªã®ã§ã€
    ChatOpenAI ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æ™‚ã« proxies ãŒæ¸¡ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
    """
    print(">>> [load_llm] ChatOpenAI is from:", ChatOpenAI)

    preset = os.getenv("MODEL_PRESET", "auto").lower()
    max_new_tokens = int(os.getenv("MAX_NEW_TOKENS", 256))
    api_key = os.environ.get("OPENAI_API_KEY")

    # auto/heavy ã®å ´åˆã€API ã‚­ãƒ¼å¿…é ˆãƒã‚§ãƒƒã‚¯
    if preset in ("auto", "heavy"):
        if not api_key or not api_key.startswith("sk-"):
            raise RuntimeError(
                "OPENAI_API_KEY ãŒæœªè¨­å®šã€ã¾ãŸã¯å½¢å¼ãŒä¸æ­£ã§ã™ï¼ï¼ˆsk- ã‹ã‚‰å§‹ã¾ã‚‹å€¤ãŒå¿…è¦ï¼‰"
            )

    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ç‰ˆï¼‰ã‚’ä½¿ã„ãŸã„å ´åˆ
    if preset == "light":
        llm, tokenizer, max_new_tokens = _load_local_rinna()
        return llm, tokenizer, max_new_tokens

    # heavy preset: GPT-4o ãªã©ã€OpenAI ä½¿ã†å ´åˆ
    if preset == "heavy":
        llm = ChatOpenAI(
            model_name="gpt-4o",
            temperature=0,
            openai_api_key=api_key
        )
        return llm, None, max_new_tokens

    # autoï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰: gpt-3.5-turbo
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo-0125",
        temperature=0,
        openai_api_key=api_key
    )
    return llm, None, max_new_tokens
