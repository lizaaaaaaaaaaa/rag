import os
import logging
import traceback  # â† ğŸ”§ å¿˜ã‚Œãšè¿½åŠ ï¼

from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from langchain_core.documents import Document
from rag.ingested_text import load_vectorstore, get_rag_chain

# ğŸ” .env èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
load_dotenv()

# ğŸ”‘ OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šï¼ˆCloud Runã§ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ³¨å…¥ï¼‰
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

# ğŸ”§ Loggingè¨­å®šï¼ˆCloud Loggingã«é€£æºã—ã‚„ã™ãï¼‰
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

router = APIRouter()

class ChatRequest(BaseModel):
    query: str

@router.post("/")
async def chat_endpoint(request: ChatRequest):
    try:
        query = request.query

        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¨RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        vectorstore = load_vectorstore()
        rag_chain = get_rag_chain(vectorstore, return_source=True, question=query)
        result = rag_chain.invoke({"query": query})

        # ã‚½ãƒ¼ã‚¹ã‚’JSONåŒ–å¯èƒ½ãªå½¢ã«å¤‰æ›
        sources = []
        for doc in result.get("source_documents", []):
            if isinstance(doc, Document):
                sources.append({
                    "page_content": str(doc.page_content),
                    "metadata": {k: str(v) for k, v in doc.metadata.items()}
                })
            else:
                sources.append(str(doc))

        return JSONResponse(content={
            "answer": str(result.get("result", "")),
            "sources": sources
        })

    except Exception as e:
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail="Internal Server Error")

