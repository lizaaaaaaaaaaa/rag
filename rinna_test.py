from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "rinna/japanese-gpt-neox-3.6b-instruction-ppo"

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_id)

# æ¨è«–ç”¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# è³ªå•ã‚’é€ã£ã¦ã¿ã‚‹
input_text = "ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ï¼šæ—¥æœ¬èªã®RAGã‚·ã‚¹ãƒ†ãƒ ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
output = generator(input_text, max_new_tokens=100, do_sample=True)

# çµæœã‚’è¡¨ç¤º
print("ğŸ§  å›ç­”:", output[0]["generated_text"])
