import os
import logging
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from sentence_transformers import SentenceTransformer

from llm.llm_runner import load_llm

# --- .envèª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç”¨ï¼‰ ---
if Path(".env").exists():
    load_dotenv()

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

VECTOR_DIR = "rag/vectorstore"
INDEX_NAME = "index"

# ---- NEW: å¿…è¦ãªæ™‚ã ã‘APIã‚­ãƒ¼å–å¾—ï¼ˆprintã§ãƒ‡ãƒãƒƒã‚°è¿½åŠ ï¼ï¼‰ ----
def get_openai_api_key():
    # ã“ã“ã§ã€Œos.environ.getã€ã§ã®å€¤ã‚’å¿…ãšprintï¼ï¼ˆCloud Runã§ã‚‚ãƒ­ã‚°å‡ºã‚‹ï¼‰
    print("[DEBUG] get_openai_api_key: os.environ.get('OPENAI_API_KEY') =", os.environ.get('OPENAI_API_KEY'))
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        print("[ERROR] rag/ingested_text.py: OPENAI_API_KEYãŒæœªè¨­å®šã§ã™ï¼")
        # raise RuntimeError("OPENAI_API_KEYãŒæœªè¨­å®šã®ã¾ã¾ã§ã™")  # å¿…è¦ãªã‚‰å¼·åˆ¶åœæ­¢
    else:
        print("[DEBUG] rag/ingested_text.py: OPENAI_API_KEY =", key[:5], "****")
    return key

class MyEmbedding(Embeddings):
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=True).tolist()

    def embed_query(self, text):
        return self.model.encode(text).tolist()

def ingest_pdf_to_vectorstore(pdf_path: str):
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    documents = splitter.split_documents(docs)
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")

    index_path = os.path.join(VECTOR_DIR, f"{INDEX_NAME}.faiss")
    if os.path.exists(index_path):
        vectorstore = FAISS.load_local(
            VECTOR_DIR, embeddings, index_name=INDEX_NAME, allow_dangerous_deserialization=True
        )
        vectorstore.add_documents(documents)
    else:
        vectorstore = FAISS.from_documents(documents, embeddings)

    vectorstore.save_local(VECTOR_DIR, index_name=INDEX_NAME)
    logger.info("âœ… %s ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ ä¿å­˜ã—ã¾ã—ãŸ", os.path.basename(pdf_path))

def load_vectorstore():
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")
    index_path = os.path.join(VECTOR_DIR, f"{INDEX_NAME}.faiss")
    if not os.path.exists(index_path):
        raise FileNotFoundError(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆ{index_path}ï¼‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚PDFå–ã‚Šè¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")
    return FAISS.load_local(
        VECTOR_DIR, embeddings, index_name=INDEX_NAME, allow_dangerous_deserialization=True
    )

def get_rag_chain(vectorstore, return_source: bool = True, question: str = ""):
    # ---- å¿…è¦ãªæ™‚ã ã‘APIã‚­ãƒ¼å–å¾—ãƒ»ã‚»ãƒƒãƒˆ ----
    import openai
    openai.api_key = get_openai_api_key()
    llm, tokenizer, max_tokens = load_llm()
    logger.info("ğŸ” get_rag_chain - preset LLM = %s", type(llm).__name__)

    try:
        with open("rag/prompt_template.txt", encoding="utf-8") as f:
            prompt_str = f.read()
    except Exception:
        prompt_str = """\
{context}

è³ªå•: {question}
AIã¨ã—ã¦åˆ†ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_str
    )

    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=return_source,
        chain_type_kwargs={"prompt": prompt}
    )
