import os
import logging
import sys
from pathlib import Path

from google.cloud import storage
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from sentence_transformers import SentenceTransformer

from llm.llm_runner import load_llm

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ GCS ãƒã‚±ãƒƒãƒˆåã‚’å–å¾—
GCS_BUCKET = os.environ.get("GCS_BUCKET_NAME", "")
# ãƒã‚±ãƒƒãƒˆå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰ã‚’å›ºå®š
GCS_VEC_DIR = "vectorstore"  # ãƒã‚±ãƒƒãƒˆç›´ä¸‹ã« "vectorstore/index.faiss" ã‚’ç½®ã

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ä¸€æ™‚ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ‘ã‚¹ï¼‰
LOCAL_VECTOR_DIR = "rag/vectorstore"
INDEX_NAME = "index"


def _get_gcs_client():
    """
    GCS ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è¿”ã™ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚
    Cloud Run ä¸Šã‚„ãƒ­ãƒ¼ã‚«ãƒ« .env ã‹ã‚‰
    GOOGLE_APPLICATION_CREDENTIALS ãŒåŠ¹ã„ã¦ã„ã‚Œã°ã€ã“ã®ã¾ã¾ã§ OKã€‚
    """
    return storage.Client()


def upload_vectorstore_to_gcs(local_dir: str):
    """
    local_dir é…ä¸‹ã«ç”Ÿæˆã•ã‚ŒãŸ index.faiss / index.pkl ã‚’
    â†’ GCS_BUCKET/vectorstore/index.faiss / index.pkl ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    """
    if not GCS_BUCKET:
        logger.error("GCS_BUCKET_NAME ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    client = _get_gcs_client()
    bucket = client.bucket(GCS_BUCKET)

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹æˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
    for fname in (f"{INDEX_NAME}.faiss", f"{INDEX_NAME}.pkl"):
        local_path = os.path.join(local_dir, fname)
        if os.path.exists(local_path):
            blob_path = f"{GCS_VEC_DIR}/{fname}"
            blob = bucket.blob(blob_path)
            blob.upload_from_filename(local_path)
            logger.info(f"âœ… GCS ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: gs://{GCS_BUCKET}/{blob_path}")
        else:
            # *.pkl ã¯å­˜åœ¨ã—ãªã„ã‚±ãƒ¼ã‚¹ã‚‚ã‚ã‚‹ã®ã§ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã inform
            logger.debug(f"ãƒ­ãƒ¼ã‚«ãƒ«ã« {local_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


def download_vectorstore_from_gcs(local_dir: str):
    """
    GCS_BUCKET/vectorstore/index.faiss, index.pkl ã‚’
    â†’ local_dir é…ä¸‹ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    å­˜åœ¨ã—ãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„ã€‚ï¼ˆæ¬¡ã« load_vectorstore() ã§ä¾‹å¤–ãŒç™ºç”Ÿã™ã‚‹ã‚ˆã†ã«ï¼‰
    """
    if not GCS_BUCKET:
        logger.error("GCS_BUCKET_NAME ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    client = _get_gcs_client()
    bucket = client.bucket(GCS_BUCKET)

    os.makedirs(local_dir, exist_ok=True)

    for fname in (f"{INDEX_NAME}.faiss", f"{INDEX_NAME}.pkl"):
        blob_path = f"{GCS_VEC_DIR}/{fname}"
        blob = bucket.blob(blob_path)
        local_path = os.path.join(local_dir, fname)

        if blob.exists():
            blob.download_to_filename(local_path)
            logger.info(f"âœ… GCS ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {blob_path} â†’ {local_path}")
        else:
            logger.debug(f"GCS ã« {blob_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


def get_openai_api_key():
    """
    ã‚‚ã¨ã‚‚ã¨ã® get_openai_api_key() ãƒ­ã‚°å‡ºåŠ›ã¯ãã®ã¾ã¾ã€‚
    """
    print("=== get_openai_api_keyãŒå‘¼ã°ã‚ŒãŸ ===", os.getenv("OPENAI_API_KEY"))
    sys.stdout.flush()
    logger.warning("=== get_openai_api_keyãŒå‘¼ã°ã‚ŒãŸ === %s", os.getenv("OPENAI_API_KEY"))

    key_env = os.environ.get("OPENAI_API_KEY")
    print("[DEBUG] get_openai_api_key: os.environ.get('OPENAI_API_KEY') =", key_env)
    sys.stdout.flush()
    logger.warning(f"[DEBUG] get_openai_api_key: os.environ.get('OPENAI_API_KEY') = {key_env}")

    key = os.getenv("OPENAI_API_KEY")
    if not key:
        print("[ERROR] rag/ingested_text.py: OPENAI_API_KEYãŒæœªè¨­å®šã§ã™ï¼")
        sys.stdout.flush()  
        logger.error("[ERROR] rag/ingested_text.py: OPENAI_API_KEYãŒæœªè¨­å®šã§ã™ï¼")
    else:
        print("[DEBUG] rag/ingested_text.py: OPENAI_API_KEY =", key[:5], "****")
        sys.stdout.flush()
        logger.info(f"[DEBUG] rag/ingested_text.py: OPENAI_API_KEY = {key[:5]} ****")
    return key


class MyEmbedding(Embeddings):
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=True).tolist()

    def embed_query(self, text):
        return self.model.encode(text).tolist()


def ingest_pdf_to_vectorstore(pdf_path: str):
    """
    1) PDF ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ FAISS ã‚’ä½œæˆï¼š
       â†’ 'rag/vectorstore/index.faiss' & 'index.pkl' ãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚
    2) ãƒ­ãƒ¼ã‚«ãƒ«é…ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ GCS ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‚
    """
    # â”€â”€ PDFèª­ã¿è¾¼ã¿â†’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    documents = splitter.split_documents(docs)

    # â”€â”€ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")

    # â”€â”€ ãƒ­ãƒ¼ã‚«ãƒ«ã® vectorstore ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ç¢ºèª
    os.makedirs(LOCAL_VECTOR_DIR, exist_ok=True)
    index_path = os.path.join(LOCAL_VECTOR_DIR, f"{INDEX_NAME}.faiss")

    if os.path.exists(index_path):
        # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚“ã§è¿½åŠ 
        # allow_dangerous_deserializationã‚’æ˜ç¤ºçš„ã«Trueã«è¨­å®š
        vectorstore = FAISS.load_local(
            LOCAL_VECTOR_DIR, embeddings,
            index_name=INDEX_NAME,
            allow_dangerous_deserialization=True
        )
        vectorstore.add_documents(documents)
    else:
        # æ–°è¦ä½œæˆ
        vectorstore = FAISS.from_documents(documents, embeddings)

    # â”€â”€ ä¿å­˜ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã« index.faiss & index.pkl ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰
    vectorstore.save_local(LOCAL_VECTOR_DIR, index_name=INDEX_NAME)
    logger.info("âœ… %s ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ ä¿å­˜ã—ã¾ã—ãŸ", os.path.basename(pdf_path))

    # â”€â”€ ãƒ­ãƒ¼ã‚«ãƒ«ã«ã§ããŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ GCS ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    try:
        upload_vectorstore_to_gcs(LOCAL_VECTOR_DIR)
    except Exception as e:
        logger.error("âŒ GCS ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ä¾‹å¤–: %s", e)
        # å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã‚’æ­¢ã‚ãšã«ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¾ã¾ã«ã—ã¦ãŠã

    # è¿½åŠ ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’è¿”ã™
    return len(documents)


def load_vectorstore():
    """
    1) ã¾ãš GCS ã‹ã‚‰æœ€æ–°ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ã€‚
    2) ãƒ­ãƒ¼ã‚«ãƒ«ã« index.faiss ãŒãªã‘ã‚Œã°ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ã€‚
    3) FAISS.load_local(... ) ã—ã¦ VectorStore ã‚’è¿”ã™ã€‚
    """
    # â”€â”€ æœ€æ–°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ GCS ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    try:
        download_vectorstore_from_gcs(LOCAL_VECTOR_DIR)
    except Exception as e:
        logger.error("âŒ GCS ã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«ä¾‹å¤–: %s", e)
        # ã“ã“ã§æ­¢ã‚ãšã«ç¶šè¡Œã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®æœ‰ç„¡ã«ä»»ã›ã‚‹

    # â”€â”€ å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    index_path = os.path.join(LOCAL_VECTOR_DIR, f"{INDEX_NAME}.faiss")
    if not os.path.exists(index_path):
        # åˆå›èµ·å‹•æ™‚ã¯ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ãªã„ã®ã§ã€ç©ºã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        logger.warning("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç©ºã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚")
        embeddings = MyEmbedding("intfloat/multilingual-e5-small")
        from langchain.schema import Document
        dummy_doc = Document(page_content="åˆæœŸåŒ–ç”¨ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ", metadata={"source": "init", "page": 0})
        vectorstore = FAISS.from_documents([dummy_doc], embeddings)
        vectorstore.save_local(LOCAL_VECTOR_DIR, index_name=INDEX_NAME)
        # GCSã«ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        try:
            upload_vectorstore_to_gcs(LOCAL_VECTOR_DIR)
        except Exception as e:
            logger.error("åˆæœŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: %s", e)
        return vectorstore

    # â”€â”€ Embeddings ã‚’å†æ§‹ç¯‰
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")
    # allow_dangerous_deserializationã‚’æ˜ç¤ºçš„ã«Trueã«è¨­å®šï¼ˆé‡è¦ï¼ï¼‰
    return FAISS.load_local(
        LOCAL_VECTOR_DIR, embeddings,
        index_name=INDEX_NAME,
        allow_dangerous_deserialization=True
    )


def get_rag_chain(vectorstore, return_source: bool = True, question: str = ""):
    """
    RAG ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã—ã¦è¿”ã™ã€‚ã‚‚ã¨ã‚‚ã¨ã®å®Ÿè£…ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
    """
    print("=== get_rag_chainãŒå‘¼ã°ã‚ŒãŸ ===")
    sys.stdout.flush()
    logger.warning("=== get_rag_chainãŒå‘¼ã°ã‚ŒãŸ ===")

    try:
        import openai
        openai.api_key = get_openai_api_key()
    except Exception as e:
        print("!!! get_rag_chainå†…ã§ä¾‹å¤–:", e)
        sys.stdout.flush()
        logger.error("get_rag_chainå†…ã§ä¾‹å¤–: %s", e)
        raise

    llm, tokenizer, max_tokens = load_llm()
    logger.info("ğŸ” get_rag_chain - preset LLM = %s", type(llm).__name__)

    try:
        with open("rag/prompt_template.txt", encoding="utf-8") as f:
            prompt_str = f.read()
    except Exception:
        prompt_str = """\
{context}

è³ªå•: {question}
AIã¨ã—ã¦åˆ†ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_str
    )

    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=return_source,
        chain_type_kwargs={"prompt": prompt}
    )