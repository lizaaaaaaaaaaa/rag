import os
import sys
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
from langchain_core.embeddings import Embeddings

# GCSè¨­å®š
try:
    from google.cloud import storage
    GCS_BUCKET = os.environ.get("GCS_BUCKET_NAME", "")
    GCS_VEC_DIR = "vectorstore"
    HAS_GCS = bool(GCS_BUCKET)
except ImportError:
    HAS_GCS = False
    GCS_BUCKET = ""

VECTOR_DIR = "rag/vectorstore"
INDEX_NAME = "index"


class MyEmbedding(Embeddings):
    """ingested_text.pyã¨åŒã˜Embeddingã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨"""
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=True).tolist()

    def embed_query(self, text):
        return self.model.encode(text).tolist()


def upload_to_gcs(local_dir: str):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    if not HAS_GCS:
        print("GCSè¨­å®šãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
    
    try:
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET)
        
        for fname in (f"{INDEX_NAME}.faiss", f"{INDEX_NAME}.pkl"):
            local_path = os.path.join(local_dir, fname)
            if os.path.exists(local_path):
                blob_path = f"{GCS_VEC_DIR}/{fname}"
                blob = bucket.blob(blob_path)
                blob.upload_from_filename(local_path)
                print(f"âœ… GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: gs://{GCS_BUCKET}/{blob_path}")
    except Exception as e:
        print(f"âš ï¸ GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")


def download_from_gcs(local_dir: str):
    """GCSã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if not HAS_GCS:
        return False
    
    try:
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET)
        os.makedirs(local_dir, exist_ok=True)
        
        downloaded = False
        for fname in (f"{INDEX_NAME}.faiss", f"{INDEX_NAME}.pkl"):
            blob_path = f"{GCS_VEC_DIR}/{fname}"
            blob = bucket.blob(blob_path)
            local_path = os.path.join(local_dir, fname)
            
            if blob.exists():
                blob.download_to_filename(local_path)
                print(f"âœ… GCSãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {blob_path}")
                downloaded = True
        return downloaded
    except Exception as e:
        print(f"âš ï¸ GCSãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def create_initial_vectorstore():
    """åˆæœŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ"""
    print("ğŸ”§ åˆæœŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(VECTOR_DIR, exist_ok=True)
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆingested_text.pyã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")
    
    # ãƒ€ãƒŸãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    dummy_docs = [
        Document(
            page_content="ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯RAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã€è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
            metadata={"source": "system_init.pdf", "page": 1}
        ),
        Document(
            page_content="PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã€ãã®å†…å®¹ãŒãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã€è³ªå•å¿œç­”ã«æ´»ç”¨ã•ã‚Œã¾ã™ã€‚",
            metadata={"source": "system_init.pdf", "page": 2}
        ),
    ]
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
    vectorstore = FAISS.from_documents(dummy_docs, embeddings)
    vectorstore.save_local(VECTOR_DIR, index_name=INDEX_NAME)
    
    print("âœ… åˆæœŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_to_gcs(VECTOR_DIR)
    
    return vectorstore


# ğŸ”¹ PDFèª­ã¿è¾¼ã¿â†’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç™»éŒ²
def ingest_pdf_to_vectorstore(pdf_path: str):
    """PDFã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ """
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    documents = splitter.split_documents(docs)

    # MyEmbeddingã‚’ä½¿ç”¨ï¼ˆingested_text.pyã¨çµ±ä¸€ï¼‰
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ‘ã‚¹
    index_path = os.path.join(VECTOR_DIR, f"{INDEX_NAME}.faiss")
    
    if os.path.exists(index_path):
        # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
        vectorstore = FAISS.load_local(
            VECTOR_DIR, embeddings, 
            index_name=INDEX_NAME, 
            allow_dangerous_deserialization=True
        )
        vectorstore.add_documents(documents)
    else:
        # æ–°è¦ä½œæˆ
        vectorstore = FAISS.from_documents(documents, embeddings)

    # ä¿å­˜
    vectorstore.save_local(VECTOR_DIR, index_name=INDEX_NAME)
    print(f"âœ… {os.path.basename(pdf_path)} ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    upload_to_gcs(VECTOR_DIR)
    
    return len(documents)


# ğŸ”¹ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿
def load_vectorstore():
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰"""
    # ã¾ãšGCSã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
    if HAS_GCS:
        download_from_gcs(VECTOR_DIR)
    
    index_path = os.path.join(VECTOR_DIR, f"{INDEX_NAME}.faiss")
    
    if not os.path.exists(index_path):
        print("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆæœŸåŒ–ã—ã¾ã™...")
        return create_initial_vectorstore()
    
    # MyEmbeddingã‚’ä½¿ç”¨
    embeddings = MyEmbedding("intfloat/multilingual-e5-small")
    return FAISS.load_local(
        VECTOR_DIR, embeddings, 
        index_name=INDEX_NAME, 
        allow_dangerous_deserialization=True
    )


# ğŸ”¹ RAGãƒã‚§ãƒ¼ãƒ³ç”Ÿæˆ
def get_rag_chain(vectorstore, return_source=True):
    """RAGãƒã‚§ãƒ¼ãƒ³ã‚’ç”Ÿæˆï¼ˆOpenAI APIã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ï¼‰"""
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    use_local = os.environ.get("USE_LOCAL_LLM", "false").lower() == "true"
    
    if use_local or not openai_api_key:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆRinnaï¼‰ã‚’ä½¿ç”¨
        print("ğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆRinnaï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™...")
        model_id = "rinna/japanese-gpt-neox-3.6b-instruction-ppo"
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            use_fast=False,
            trust_remote_code=True
        )
        print("âœ… Tokenizer loaded:", tokenizer.__class__)
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            trust_remote_code=True
        )
        
        pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
        llm = HuggingFacePipeline(pipeline=pipe)
    else:
        # OpenAI APIã‚’ä½¿ç”¨
        print("ğŸ¤– OpenAI APIã‚’ä½¿ç”¨ã—ã¾ã™...")
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo-0125",
            temperature=0,
            openai_api_key=openai_api_key
        )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    from langchain.prompts import PromptTemplate
    try:
        with open("rag/prompt_template.txt", encoding="utf-8") as f:
            prompt_str = f.read()
    except:
        prompt_str = """{context}

è³ªå•: {question}
AIã¨ã—ã¦åˆ†ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚"""
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_str
    )
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=return_source,
        chain_type_kwargs={"prompt": prompt}
    )


# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†ï¼ˆåˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦å®Ÿè¡Œã™ã‚‹å ´åˆï¼‰
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆæœŸåŒ–ãƒ»ç®¡ç†ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--init", action="store_true", help="åˆæœŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ")
    parser.add_argument("--pdf", type=str, help="PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ")
    parser.add_argument("--test", action="store_true", help="ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å‹•ä½œãƒ†ã‚¹ãƒˆ")
    
    args = parser.parse_args()
    
    if args.init:
        # åˆæœŸåŒ–
        create_initial_vectorstore()
    elif args.pdf:
        # PDFè¿½åŠ 
        if os.path.exists(args.pdf):
            ingest_pdf_to_vectorstore(args.pdf)
        else:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.pdf}")
    elif args.test:
        # ãƒ†ã‚¹ãƒˆ
        try:
            vectorstore = load_vectorstore()
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®èª­ã¿è¾¼ã¿æˆåŠŸ")
            
            # ç°¡å˜ãªæ¤œç´¢ãƒ†ã‚¹ãƒˆ
            results = vectorstore.similarity_search("ã‚·ã‚¹ãƒ†ãƒ ", k=1)
            if results:
                print(f"ğŸ“ æ¤œç´¢çµæœ: {results[0].page_content[:100]}...")
            else:
                print("âš ï¸ æ¤œç´¢çµæœãªã—")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: åˆæœŸåŒ–ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(os.path.join(VECTOR_DIR, f"{INDEX_NAME}.faiss")):
            print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆæœŸåŒ–ã—ã¾ã™...")
            create_initial_vectorstore()
        else:
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")