# âœ… rinnaç”¨ã«èª¿æ•´ã•ã‚ŒãŸRAGå®Œæˆç‰ˆ
# ãƒ•ã‚¡ã‚¤ãƒ«1ï¼šrag/ingested_text.py
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

VECTOR_DIR = "rag/vectorstore"
INDEX_NAME = "index"

# ğŸ”¹ PDFèª­ã¿è¾¼ã¿â†’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç™»éŒ²
def ingest_pdf_to_vectorstore(pdf_path: str):
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    documents = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")

    if os.path.exists(os.path.join(VECTOR_DIR, f"{INDEX_NAME}.faiss")):
        vectorstore = FAISS.load_local(VECTOR_DIR, embeddings, index_name=INDEX_NAME, allow_dangerous_deserialization=True)
        vectorstore.add_documents(documents)
    else:
        vectorstore = FAISS.from_documents(documents, embeddings)

    vectorstore.save_local(VECTOR_DIR, index_name=INDEX_NAME)
    print(f"âœ… {os.path.basename(pdf_path)} ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã—ã¾ã—ãŸ")

# ğŸ”¹ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿
def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")
    return FAISS.load_local(VECTOR_DIR, embeddings, index_name=INDEX_NAME, allow_dangerous_deserialization=True)

# ğŸ”¹ RAGãƒã‚§ãƒ¼ãƒ³ç”Ÿæˆï¼ˆrinnaãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œï¼‰
def get_rag_chain(vectorstore, return_source=True):
    model_id = "rinna/japanese-gpt-neox-3.6b-instruction-ppo"
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_id)

    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
    llm = HuggingFacePipeline(pipeline=pipe)

    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=return_source,
    )


# ãƒ•ã‚¡ã‚¤ãƒ«2ï¼špages/1_upload.py
import streamlit as st
import os
from rag.ingested_text import ingest_pdf_to_vectorstore, load_vectorstore, get_rag_chain
from tempfile import NamedTemporaryFile

st.set_page_config(page_title="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & RAGè³ªå•", layout="wide")

st.title("ğŸ“¤ PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ğŸ’¬ RAGè³ªå•")
st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFã®å†…å®¹ã‹ã‚‰è³ªå•ã§ãã¾ã™")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["pdf"])

if uploaded_file is not None:
    with NamedTemporaryFile(delete=False, suffix=".pdf", dir="uploaded_docs") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    st.success("âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸï¼")

    try:
        ingest_pdf_to_vectorstore(tmp_path)
        st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å–ã‚Šè¾¼ã¿å®Œäº†ï¼")
    except Exception as e:
        st.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()

    st.subheader("ğŸ’¬ è³ªå•ã—ã¦ã¿ã‚ˆã†ï¼")
    question = st.text_input("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFã®å†…å®¹ã«ã¤ã„ã¦è³ªå•")

    if question:
        try:
            vectorstore = load_vectorstore()
            rag_chain = get_rag_chain(vectorstore, return_source=True)
            result = rag_chain.invoke({"query": question})

            st.write(f"ğŸ“˜ å›ç­”: {result.get('result', 'âŒ å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')}")

            if result.get("source_documents"):
                st.write("ğŸ“ å‡ºå…¸:")
                for doc in result["source_documents"]:
                    source = doc.metadata.get("source", "ä¸æ˜")
                    page = doc.metadata.get("page", "?")
                    st.write(f"- {source} (p{page})")
        except Exception as e:
            st.error(f"RAGå¿œç­”ã‚¨ãƒ©ãƒ¼: {e}")
